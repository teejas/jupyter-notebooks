{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23929029-39ca-4b0a-96c5-94df09c264b6",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "I want to continue in the same vein as my `twitbotnet` projectm where I tried to essentially scrape data from Twitter via the API to generate a graphical representation of peoples immediate social network (`depth=1`).\n",
    "\n",
    "Now that the Muskrat owns Twitter and is laying off folks en masse I have diminishing faith in the maintenance and stability of the Twitter API. I want to turn my attention to something more self-reliant and free of rate-limiting frustrations.\n",
    "\n",
    "I'm going to fuse the `twitbotnet` and what little there was of `crisiswatch` (and put it in the `crisiswatch` [repo](https://github.com/teejas/crisiswatch)). So this new project will be a fleet of web scrapers that specifically collect information from sites which record humanitarian crises (with a focus on Southeast Asia to start with)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e893dad-6110-4053-a997-01e218da06fd",
   "metadata": {},
   "source": [
    "## Web Crawling\n",
    "\n",
    "The starting point is to certainly build some web crawlers that can generate a link tree of URL's that are important to this project. The URL's I'm interested in are those for websites with content about humanitarian crises such as Amnesty International and [APDP](https://apdpkashmir.com/press/). I'd like to start with the crawling to generate a link tree then address the scraping of each website. This makes more sense because some websites might be able to be scraped in the same manner, and I want to avoid redundant code.\n",
    "\n",
    "Thinking about it, Google has already done the web crawling work for me right? I can just get a list of links from Google for the result \"humanitarian crises\". I'll do some messing around to refine the search.\n",
    "\n",
    "Using [Google Advanced Search](https://www.google.com/advanced_search) to narrow things to \"humanitarian crises in India\" within the last year, and there are \"top crises to look out for\" lists that would be a good starting point for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a762bfa-88c3-4960-9bcd-275f8e728ebb",
   "metadata": {},
   "source": [
    "# End State\n",
    "\n",
    "It's a little simple but my first objective here is to just create a single repository for as many *recent* humanitarian crisis reports there are from *specifically* the Southeast Asia region. Having this geographical and topical focus of scope should limit how many sites and data there is to scrape. I don't want to have a project that just spins out of control, so I will further limit the scope if the amount of data or number of links is way too high.\n",
    "\n",
    "After this I can build dashboards on top of the database to represent geographically where crises are being reported from. Who the belligerints and non-belligerints are. Do some basic NLP analysis of the written conten using OpenAI. There's a lot of options here but it all begins with a strong and reliable dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
